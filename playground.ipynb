{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "Data format:\n",
    "\n",
    "|id|word_seq|tag_seq|\n",
    "|:--|:--|:--|\n",
    "|index of the sentence|tokenized words|corresponding NER tags|\n",
    "|0|`[\"protection\", \"calves\", ...]`|`[\"O\", \"LIVESTOCK\", ...]`|\n",
    "|1|`[\"prevent\", \"diarrhea\",...]` |`[\"O\", \"DISEASE_OR_SYNDROME\", ...]`|\n",
    "|...|...|...|\n",
    "\n",
    "\n",
    "\n",
    "There are 64 categories of NER tags (plus 1 padding token).\n",
    "\n",
    "The ground-truth tags are provided for the training and testing set, while being omitted in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "train_dict = pkl.load(open(\"data/train.pkl\", \"rb\"))\n",
    "val_dict = pkl.load(open(\"data/val.pkl\", \"rb\"))\n",
    "test_dict = pkl.load(open(\"data/test.pkl\", \"rb\"))\n",
    "print(\"keys in train_dict:\", train_dict.keys())\n",
    "print(\"keys in val_dict:\", val_dict.keys())\n",
    "print(\"keys in test_dict:\", test_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an entry of the dataset\n",
    "print(\"index:\", train_dict[\"id\"][0])\n",
    "print(*zip(train_dict[\"word_seq\"][0], train_dict[\"tag_seq\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the NER tags:\n",
    "from itertools import chain\n",
    "print(\"count of the NER tags:\", len(set(chain(*train_dict[\"tag_seq\"]))))\n",
    "print(\"all the NER tags:\", set(chain(*train_dict[\"tag_seq\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare word vocab and tag vocab\n",
    "\n",
    "vocab_dict = {'_unk_': 0, '_w_pad_': 1}\n",
    "\n",
    "for doc in train_dict['word_seq']:\n",
    "    for word in doc:\n",
    "        if(word not in vocab_dict):\n",
    "            vocab_dict[word] = len(vocab_dict)\n",
    "\n",
    "tag_dict = {'_t_pad_': 0} # add a padding token\n",
    "\n",
    "for tag_seq in train_dict['tag_seq']:\n",
    "    for tag in tag_seq:\n",
    "        if(tag not in tag_dict):\n",
    "            tag_dict[tag] = len(tag_dict)\n",
    "word2idx = vocab_dict\n",
    "idx2word = {v:k for k,v in word2idx.items()}\n",
    "tag2idx = tag_dict\n",
    "idx2tag = {v:k for k,v in tag2idx.items()}            \n",
    "\n",
    "print(\"size of word vocab:\", len(vocab_dict), \"size of tag_dict:\", len(tag_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of a sentence is set to 128\n",
    "max_sent_length = 128\n",
    "\n",
    "train_tokens = np.array([[word2idx[w] for w in doc] for doc in train_dict['word_seq']])\n",
    "val_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in val_dict['word_seq']])\n",
    "test_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in test_dict['word_seq']])\n",
    "\n",
    "\n",
    "train_tags = [[tag2idx[t] for t in t_seq] for t_seq in train_dict['tag_seq']]\n",
    "train_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in train_tags])\n",
    "\n",
    "val_tags = [[tag2idx[t] for t in t_seq] for t_seq in val_dict['tag_seq']]\n",
    "val_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in val_tags])\n",
    "\n",
    "# we don't have test tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training size:\", train_tokens.shape, \"tag size:\", train_tags.shape)\n",
    "print(\"validating size:\", val_tokens.shape, \"tag size:\", val_tags.shape)\n",
    "print(train_dict['word_seq'][:2])\n",
    "print(np.array(train_dict['word_seq']).shape)\n",
    "print(train_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of training instance and training tags.\n",
    "print(train_tokens[0,:10], np.argmax(train_tags[0, :10, :], axis=1))\n",
    "print(train_dict['word_seq'][0][:10], train_dict['tag_seq'][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two simple models and codes for evaluation\n",
    "\n",
    "1. Predict all the tags as \"O\".\n",
    "2. Random guess\n",
    "\n",
    "You could use the `calc_accuracy` function to evaluate the accuracy of your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided function to test accuracy\n",
    "# You could check the validation accuracy to select the best of your models\n",
    "def calc_accuracy(preds, tags, padding_id=\"_t_pad_\"):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            preds (np.narray): (num_data, length_sentence)\n",
    "            tags  (np.narray): (num_data, length_sentence)\n",
    "        Output:\n",
    "            Proportion of correct prediction. The padding tokens are filtered out.\n",
    "    \"\"\"\n",
    "    preds_flatten = preds.flatten()\n",
    "    tags_flatten = tags.flatten()\n",
    "    non_padding_idx = np.where(tags_flatten!=padding_id)[0]\n",
    "    \n",
    "    return sum(preds_flatten[non_padding_idx]==tags_flatten[non_padding_idx])/len(non_padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on the training set\n",
    "train_tags_by_idx = np.argmax(train_tags, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "\n",
    "print(calc_accuracy(train_labels, train_labels))\n",
    "\n",
    "# Predict all labels as \"O\"\n",
    "# np.ones will create a matrix that contain all ones and idx2tag will change 1 to O \n",
    "\n",
    "baseline1_train_preds = np.array([[idx2tag[p] for p in preds] for preds in np.ones(train_labels.shape)])\n",
    "print(baseline1_train_preds.shape)\n",
    "print( train_labels.shape)\n",
    "print(\"baseline 1, make all predictions as 1. Acc:\", \n",
    "      calc_accuracy(baseline1_train_preds, \n",
    "                    train_labels))\n",
    "\n",
    "# Randomly guess labels.\n",
    "baseline2_train_preds = np.array([[idx2tag[p] for p in preds] for preds in np.random.randint(1, len(tag_dict), train_labels.shape)]) \n",
    "print(\"baseline 2, Random guess. Acc:\", \n",
    "      calc_accuracy(baseline2_train_preds,\n",
    "                    train_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_dict['word_seq'] \n",
    "train_y = train_dict ['tag_seq'] \n",
    "valid_x = val_dict ['word_seq']\n",
    "valid_y = val_dict[ 'tag_seq'] \n",
    "test_x = test_dict['word_seq']\n",
    "print(np.array(train_x).shape , np.array(train_y).shape, np.array(valid_x).shape, np.array(valid_y).shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import kashgari\n",
    "from kashgari.tasks.labeling import BiGRU_Model, BiGRU_CRF_Model, BiLSTM_CRF_Model,  BiLSTM_Model ,CNN_LSTM_Model \n",
    "# from kashgari.embeddings import TransformerEmbedding\n",
    "import os \n",
    "\n",
    "def getModel(name ) : \n",
    "    return {\n",
    "        'BiGRU_Model' : BiGRU_Model () , \n",
    "        'BiGRU_CRF_Model' : BiGRU_CRF_Model() , \n",
    "        'BiLSTM_CRF_Model' : BiLSTM_CRF_Model ( ) , \n",
    "        'BiLSTM_Model' : BiLSTM_Model() , \n",
    "        'CNN_LSTM_Model' : CNN_LSTM_Model() , \n",
    "    }[name]\n",
    "\n",
    "def getModelClass(name): \n",
    "    return {\n",
    "        'BiGRU_Model' : BiGRU_Model , \n",
    "        'BiGRU_CRF_Model' : BiGRU_CRF_Model, \n",
    "        'BiLSTM_CRF_Model' : BiLSTM_CRF_Model, \n",
    "        'BiLSTM_Model' : BiLSTM_Model , \n",
    "        'CNN_LSTM_Model' : CNN_LSTM_Model , \n",
    "    }[name]\n",
    "\n",
    "models = ['BiGRU_Model' , 'BiGRU_CRF_Model', 'BiLSTM_CRF_Model' , 'BiLSTM_Model' ,'CNN_LSTM_Model' ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame(columns = [\"model_name\" , \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in [5 , 10  ] : \n",
    "    for b in [64, 128 ] : \n",
    "        for m in models : \n",
    "            # for em in embeddings : \n",
    "                model_folder = f'{m}_{b}_{e}'\n",
    "                if os.path.isdir(model_folder) : \n",
    "                    model = getModelClass(m) .load_model(model_folder)\n",
    "                else : \n",
    "                    model = BiLSTM_Model(embed) \n",
    "                    model.fit(train_x, train_y , valid_x, valid_y , batch_size=b, epochs=e)\n",
    "                    model.save(model_folder)\n",
    "                train_preds = model.predict(train_x)\n",
    "                val_preds = model.predict(valid_x)\n",
    "                train_report = model.evaluate(test_x , test_y )\n",
    "                val_report = model.evaluate(valid_x , valid_y )\n",
    "                print(f\"{model_folder} train preds : \" , calc_accuracy(np.array(train_preds) , np.array(train_y) ) ) \n",
    "                print(f\"{model_folder} valid pred : \" , calc_accuracy(np.array(val_preds) , np.array(valid_y ) ) ) \n",
    "# test_y = model.predict(test_x) \n",
    "# report = model.evaluate(test_x , real_test_y ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(report )\n",
    "# get the test pred by loading the models \n",
    "# find best f1 score \n",
    "# find best epoch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output format\n",
    "\n",
    "In this project, you should predict the NER tags for the test set tokens.\n",
    "\n",
    "The index of test set starts from 0 and ends with 2949.\n",
    "\n",
    "You should write the predictions into a .csv file, where the first column is the test indexes in ascending order, and the second column is a json format prediction list.\n",
    "\n",
    "E.g.\n",
    "\n",
    "|id|labels|\n",
    "|:--:|:--:|\n",
    "|0|`['O', 'O', 'CHEMICAL', 'VIRUS', ...]`|\n",
    "|1|`['O', 'O', 'GENE_OR_GENOME', ...]`|\n",
    "|...|...|\n",
    "\n",
    "Format requirements:\n",
    "1. The first column `id` should be an integer, in ascending order, starting from 0 and corresponding to the index in test_dict.\n",
    "2. The second column `labels` should be a dumped string using json, storing the your predictions for each token. The size of the list should be exactly 128, including padding tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For example, this is your prediction for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_numerical = np.random.randint(1, len(tag_dict), \n",
    "                                         (len(test_dict[\"id\"]), max_sent_length))\n",
    "test_preds = np.array([[idx2tag[p] for p in preds] for preds in test_preds_numerical])\n",
    "print(test_preds.shape)\n",
    "print(test_preds[0])\n",
    "\n",
    "# use the model to make the preds on test and create a matirx called test_preds_numerical\n",
    "# change the index back to tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the baseline 1 as an example, where we predict all labels as 1.\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'id': test_dict[\"id\"],\n",
    "                   'labels': [json.dumps(np.array(preds).tolist()) for preds in test_preds]})\n",
    "df.to_csv('test_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"test_preds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please make your output-format exactly the same as above\n",
    "\n",
    "You could check it by playing around with the validation set with our evaluation codes `evaluate.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_preds_numerical = np.random.randint(1, len(tag_dict), \n",
    "#                                          (len(val_dict[\"id\"]), max_sent_length))\n",
    "val_preds = np.array([[idx2tag[p] for p in preds] for preds in np.ones((len(val_dict[\"id\"]), max_sent_length))])\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'id': val_dict[\"id\"],\n",
    "                   'labels': [json.dumps(np.array(preds).tolist()) for preds in val_preds]})\n",
    "df.to_csv('val_preds.csv', index=False)\n",
    "\n",
    "from evaluate import evaluate\n",
    "\n",
    "print(\"val accuracy\", evaluate('val_preds.csv', \"data/val.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}